{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
    "word2index = {}\n",
    "ch2index = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word2index:\n",
    "            word2index[word] = len(word2index)\n",
    "            \n",
    "        for ch in word:\n",
    "            if ch not in ch2index:\n",
    "                ch2index[ch] = len(ch2index)\n",
    "\n",
    "tag2index = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tag_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tag_size)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        embeds = self.word_embedding(seq)\n",
    "        out, _ = self.lstm(embeds.view(len(seq), 1, -1))\n",
    "        out = self.hidden2tag(out.view(len(seq), -1))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = LSTMTagger(6, 16, len(word2index), len(tag2index))\n",
    "loss_f = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedTagger(nn.Module):\n",
    "    def __init__(self, w_embedding_dim, c_embedding_dim, hidden_dim1, hidden_dim2, vocab_size, character_size, tag_size):\n",
    "        super(AugmentedTagger, self).__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, w_embedding_dim)\n",
    "        self.ch_embedding = nn.Embedding(character_size, c_embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(c_embedding_dim, hidden_dim1)\n",
    "        self.lstm2 = nn.LSTM(w_embedding_dim + hidden_dim1, hidden_dim2)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim2, tag_size)\n",
    "        \n",
    "    def forward(self, seq, ch_seq):\n",
    "        w_embeds = self.word_embedding(seq)\n",
    "        w_embeds = w_embeds.view(len(seq), -1)\n",
    "        embeds = []\n",
    "        \n",
    "        for i in range(len(w_embeds)):\n",
    "            c_embeds = self.ch_embedding(torch.tensor(ch_seq[i], dtype = torch.long))\n",
    "            _, out = self.lstm1(c_embeds.view(len(ch_seq[i]), 1, -1))\n",
    "            embeds.append(torch.cat((w_embeds[i], out[1][0][0]), 0))\n",
    "        \n",
    "        embeds = torch.cat(tuple(embeds), 0)\n",
    "        out, _ = self.lstm2(embeds.view(len(seq), 1, -1))\n",
    "        out = self.hidden2tag(out.view(len(seq), -1))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = AugmentedTagger(6, 5, 10, 16, len(word2index), len(ch2index), len(tag2index))\n",
    "loss_f = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0206, -1.1109, -1.1701],\n",
      "        [-1.0233, -1.1051, -1.1731],\n",
      "        [-1.0073, -1.1193, -1.1767],\n",
      "        [-1.0184, -1.1153, -1.1679],\n",
      "        [-1.0024, -1.1194, -1.1825]])\n",
      "tensor([[-0.0322, -5.6385, -3.5694],\n",
      "        [-6.8223, -0.0142, -4.3403],\n",
      "        [-3.6461, -3.6338, -0.0539],\n",
      "        [-0.0208, -7.0147, -3.9264],\n",
      "        [-5.1633, -0.0177, -4.4388]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    seq = torch.tensor([word2index[w] for w in training_data[0][0]], dtype = torch.long)\n",
    "    ch_seq = [tuple([ch2index[ch] for ch in w]) for w in training_data[0][0]]\n",
    "    tag_scores = model(seq, ch_seq)\n",
    "\n",
    "    print(tag_scores)\n",
    "    \n",
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        seq = torch.tensor([word2index[w] for w in sentence], dtype = torch.long)\n",
    "        ch_seq = [tuple([ch2index[ch] for ch in w]) for w in sentence]\n",
    "        predicts = model(seq, ch_seq)\n",
    "        loss = loss_f(predicts, torch.tensor([tag2index[t] for t in tags], dtype = torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    seq = torch.tensor([word2index[w] for w in training_data[0][0]], dtype = torch.long)\n",
    "    ch_seq = [tuple([ch2index[ch] for ch in w]) for w in training_data[0][0]]\n",
    "    tag_scores = model(seq, ch_seq)\n",
    "\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
